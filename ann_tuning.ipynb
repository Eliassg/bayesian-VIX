{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06ffa4e-db0e-45c9-ba13-151b4bb93a43",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb18e53-0105-45b4-ad54-90305c415b1c",
   "metadata": {},
   "source": [
    "The following notebook demonstrates the tuning procedure for the ANN models. This demonstration uses 30 lags and 1-day ahead predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4dd68-072b-47c7-9e65-7545cfcd00da",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5262b-49e5-4530-aefe-6615e0b3453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135e1e3-013c-4559-b51d-92cd01473cab",
   "metadata": {},
   "source": [
    "Import tensorflow and keras packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee62ee-40ce-439a-a8ee-2aeabadc505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ef346-d566-41b3-b7f6-54ad2563d3fd",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07db583-31a7-411e-ad55-00dcaefc8ae4",
   "metadata": {},
   "source": [
    "Read the data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad60722-b8d7-444c-8c2c-33ec53bd983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"feature_sel_1_day.csv\", index_col=0, parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953394d-66ca-4a31-a0c1-a1b01e4a687c",
   "metadata": {},
   "source": [
    "Create the split using a *70% / 30%* split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8163b3-90b4-4550-b78a-7b0f605c1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "n = len(df)\n",
    "train_ind = int(n*train_size)\n",
    "train_df = df[:train_ind]\n",
    "test_df = df[train_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a3003-0d38-4d9a-b916-6f1feb55ee06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb78d4-1aee-404b-b318-91797e5661d2",
   "metadata": {},
   "source": [
    "Perform hyperparameter tuning of an ANN model using the `Keras tuner` package and Bayesian optimization.\n",
    "\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4f193-9f23-4dfb-9f6b-adf40419a737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras_tuner import HyperParameters as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03f940-b09c-4c8b-8270-e41a7f73f1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import backend as backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fb51c-555b-448e-b6cd-ae301f8882b4",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94722abe-7a12-49d8-8349-801e27f29d40",
   "metadata": {},
   "source": [
    "In order to perform a hyperparameter search using the Keras Tuner package, we have to create a function that details how the model shall be built. This entails setting the search space and creating the architecture. In our thesis we ran a separate search for each different variation of the Bayesian Neural Networks and LSTM models, but these can be condensed into one single tuning session by letting the type of prior and posterior be part of the search space. We will not demonstrate how this can be done, as this makes the search space more complex, thus confuscating the inference of model performances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e50cd-eba9-43c0-9eb4-abdbd319b48a",
   "metadata": {},
   "source": [
    "### LSTM model\n",
    "Create function for building an LSTM model for the tuning procedure.\n",
    "\n",
    "Note that the tuning procedure can be memory intensive if a GPU is utilized. To alleviate this issue, the `backend.clear_session()` command may be used. Further alterations are applied in each trial of the tuner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1da01-5a0b-4cd6-b2bd-1b1a0f99bc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_LSTM_model(hp):\n",
    "    backend.clear_session()  \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    first_layer = hp.Int(\"input_unit_neurons\", min_value=16, max_value=64, step=16)\n",
    "    model.add(LSTM(first_layer, return_sequences=True, input_shape=(input_width, num_features)))\n",
    "    \n",
    "    # Hidden\n",
    "    use_hidden_layer = hp.Boolean(\"use_hidden_layer\")\n",
    "    with hp.conditional_scope(\"use_hidden_layer\", [True]):\n",
    "        if use_hidden_layer:\n",
    "            hidden_layer = hp.Int(\"hidden_layer_neurons\", min_value=32, max_value=128, step=32)\n",
    "            model.add(LSTM(hidden_layer, return_sequences=True))\n",
    "    \n",
    "    # Final LSTM layer    \n",
    "    final_layer = hp.Int(\"last_unit_neurons\", min_value=16, max_value=128, step=16)\n",
    "    model.add(LSTM(final_layer))\n",
    "    \n",
    "    # Add dropout layer\n",
    "    model.add(Dropout(hp.Float('Dropout_rate', min_value=0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Final output layer\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # \n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    metrics = [\"mse\", \"mae\", \"mape\", tf.keras.metrics.RootMeanSquaredError()]\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate), metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad3215-5cfb-4ae6-996e-247c2c0fb30c",
   "metadata": {},
   "source": [
    "### MC Dropout model\n",
    "The equivalent model builder for a Monte Carlo Dropout model. This is very similar to the LSTM model, albeit with more dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfade0-c06c-4c64-a7f2-48f24e2748f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mc_dropout_model(hp):\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    # LSTM aspect\n",
    "    # Input layer\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(input_width, num_features)))\n",
    "\n",
    "    # First dropout layer\n",
    "    first_dropout_rate = hp.Float(\"first_dropout_rate\", min_value=0.1, max_value=0.6, step=0.1)\n",
    "    model.add(Dropout(rate=first_dropout_rate))\n",
    "  \n",
    "    # Final LSTM layer    \n",
    "    model.add(LSTM(96))\n",
    "\n",
    "    # Second dropout layer\n",
    "    second_dropout_rate = hp.Float(\"second_dropout_rate\", min_value=0.1, max_value=0.6, step=0.1)\n",
    "    model.add(Dropout(rate=second_dropout_rate))\n",
    "    \n",
    "    # Dense output\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    learning_rate =  hp.Float('learning_rate', min_value=0.0001, max_value=0.05)\n",
    "    optimizer = Adam(learning_rate=learning_rate)      \n",
    "    metrics = [\"mse\", \"mae\", \"mape\", tf.keras.metrics.RootMeanSquaredError(), tf.keras.losses.Huber()]\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b9d54-64c4-4f1a-ad55-b62b28704b5f",
   "metadata": {},
   "source": [
    "### Diagonal Normal Bayesian Neural Network\n",
    "The model builder function for a diagonal BNN model. \n",
    "\n",
    "Note the prior and posterior functions, which determines the type of probability function to be used for these two parameters. In this case, a non-trainable diagonal normal prior is employed where only the `loc` (or mean) and `scale` (or standard deviation) is specified. For the sake of simplicity a mean of zero is used, leaving only the variance as the adjustable parameter. \n",
    "\n",
    "For the posterior distribution, the rescaling factor is tuned. This parameter scales down the variance to achieve more stable training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625acbb7-be4b-4c12-80e3-ecfeb143b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bayesian_diag_normal_model(hp):\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    # LSTM aspect\n",
    "    # Input layer\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(input_width, num_features)))\n",
    "\n",
    "    # Final LSTM layer    \n",
    "    model.add(LSTM(96))\n",
    "\n",
    "    # Bayesian aspect\n",
    "    \n",
    "    # Scale of prior\n",
    "    # USING DIAGONAL NORMAL!\n",
    "    normal_scale = hp.Float(\"normal_scale\", min_value=0.1, max_value=2)\n",
    "    def prior_tune_normal(kernel_size, bias_size, dtype = None):\n",
    "        n = kernel_size + bias_size # num of params\n",
    "        sc = tf.ones(n)*normal_scale\n",
    "        return Sequential([\n",
    "            tfp.layers.DistributionLambda(\n",
    "                lambda t: tfd.MultivariateNormalDiag(loc = tf.zeros(n), scale_diag = sc)\n",
    "                )                     \n",
    "          ])\n",
    "    prior_func = prior_tune_normal\n",
    "\n",
    "    # Scale of posterior\n",
    "    posterior_scale = hp.Float('posterior_scale', min_value=0.0001, max_value=0.1)\n",
    "    def posterior_tune(kernel_size, bias_size, dtype=None):\n",
    "        n = kernel_size + bias_size\n",
    "        c = np.log(np.expm1(1.))\n",
    "        return tf.keras.Sequential([\n",
    "          tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "          tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "              tfd.Normal(loc=t[..., :n],\n",
    "                         scale = 1e-3 + posterior_scale * tf.nn.softplus(c + t[..., n:])),\n",
    "              reinterpreted_batch_ndims=1)),\n",
    "        ])\n",
    "    \n",
    "    # Give opportunity of adding one more dense variational layer\n",
    "    # Hidden DenseVariational layer\n",
    "    use_hidden_vi_layer = hp.Boolean(\"use_hidden_vi_layer\")\n",
    "    with hp.conditional_scope(\"use_hidden_vi_layer\", [True]):\n",
    "        if use_hidden_vi_layer:\n",
    "            hidden_layer_units = hp.Int(\"hidden_vi_layer_neurons\", min_value=8, max_value=64)\n",
    "            model.add(tfp.layers.DenseVariational(\n",
    "                hidden_layer_units, make_posterior_fn=posterior_tune, make_prior_fn=prior_func,\n",
    "                kl_weight=1/num_samples\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    model.add(\n",
    "        tfp.layers.DenseVariational(\n",
    "            1, make_posterior_fn=posterior_tune, make_prior_fn=prior_func, \n",
    "            kl_weight=1/num_samples\n",
    "        )    \n",
    "    )\n",
    "    \n",
    "    learning_rate =  hp.Float('learning_rate', min_value=0.0001, max_value=0.1)\n",
    "    optimizer = Adam(learning_rate=learning_rate)      \n",
    "    metrics = [\"mse\", \"mae\", \"mape\", tf.keras.metrics.RootMeanSquaredError(), tf.keras.losses.Huber()]\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240f4d9-9ef5-4293-8902-2c71f4a6298f",
   "metadata": {},
   "source": [
    "While the above demonstrates a Diagonal Normal hybrid model builder, small alterations can be made to create the model builder function for the other models:\n",
    "* If a `Laplace prior` is used, simply replace the `Normal` part of the Distribution Lambda with a `tfd.Laplace` function.\n",
    "* If a `Pure Bayesian` model is used, replace the first LSTM layers with the desired number of Dense Variational Layers. Apply the same logic as the optional hidden layer used above to search for the number of hidden layeres to utilize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e232f2-6ebf-4ff8-9e9d-6c0bbc0f2f2d",
   "metadata": {},
   "source": [
    "### Gaussian Scale Mixture Model\n",
    "The Scale Mixture model is somewhat more complicated, as the current (as of June 2023) Tensorflow Probability framework does not allow for a simple implementation of this. The next cell demonstrates how this prior can be used by extending on the base Dense Variational Class. First, a custom Dense variational Class is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dd4e7-bc01-46ed-9e6b-1ed8114bb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom DVI class able to handle scale gaussian mixture priors\n",
    "class Custom_dvi(tfp.layers.DenseVariational):\n",
    "    \"\"\"\n",
    "    An extension of the Tensorflow Probability Dense Variational Class to allow for having Gaussian Scale Mixture priors. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, comp_weight, first_scale, second_scale, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.comp_weight = comp_weight\n",
    "        self.first_scale = first_scale\n",
    "        self.second_scale = second_scale\n",
    "        self._kl_divergence_fn = _make_kl_divergence_penalty(\n",
    "        use_exact_kl=kwargs.get('kl_use_exact', False), weight=kwargs.get('kl_weight', 1335), \n",
    "        comp_weight=self.comp_weight, first_prior_scale=self.first_scale, second_prior_scale=self.second_scale\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        inputs = tf.cast(inputs, dtype, name='inputs')\n",
    "\n",
    "        q = self._posterior(inputs)\n",
    "        r = self._prior(inputs)\n",
    "        self.add_loss(\n",
    "            self._kl_divergence_fn(q, r))\n",
    "\n",
    "        w = tf.convert_to_tensor(value=q)\n",
    "        prev_units = self.input_spec.axes[-1]\n",
    "        if self.use_bias:\n",
    "            split_sizes = [prev_units * self.units, self.units]\n",
    "            kernel, bias = tf.split(w, split_sizes, axis=-1)\n",
    "        else:\n",
    "            kernel, bias = w, None\n",
    "\n",
    "        kernel = tf.reshape(kernel, shape=tf.concat([\n",
    "            tf.shape(kernel)[:-1],\n",
    "            [prev_units, self.units],\n",
    "        ], axis=0))\n",
    "        outputs = tf.matmul(inputs, kernel)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(outputs, bias)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)  # pylint: disable=not-callable\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def _make_kl_divergence_penalty(\n",
    "    use_exact_kl=False,\n",
    "    test_points_reduce_axis=(),  # `None` == \"all\"; () == \"none\".\n",
    "    test_points_fn=tf.convert_to_tensor,\n",
    "    weight=None,\n",
    "    comp_weight=None,\n",
    "    first_prior_scale=None,\n",
    "    second_prior_scale=None\n",
    "):\n",
    "    \"\"\"Creates a callable computing `KL[a,b]` from `a`, a `tfd.Distribution`.\"\"\"\n",
    "\n",
    "    if use_exact_kl:\n",
    "        kl_divergence_fn = kullback_leibler.kl_divergence\n",
    "    else:\n",
    "        def kl_divergence_fn(distribution_a, distribution_b):\n",
    "            z = test_points_fn(distribution_a)\n",
    "\n",
    "            return tf.reduce_mean(\n",
    "              distribution_a.log_prob(z) - log_mixture_prior_prob(z, comp_weight, first_prior_scale, second_prior_scale),\n",
    "              axis=test_points_reduce_axis)\n",
    "\n",
    "  # Closure over: kl_divergence_fn, weight.\n",
    "  def _fn(distribution_a, distribution_b):\n",
    "    \"\"\"Closure that computes KLDiv as a function of `a` as in `KL[a, b]`.\"\"\"\n",
    "    with tf.name_scope('kldivergence_loss'):\n",
    "        kl = kl_divergence_fn(distribution_a, distribution_b)\n",
    "        if weight is not None:\n",
    "            kl = tf.cast(weight, dtype=kl.dtype) * kl\n",
    "      # Losses appended with the model.add_loss and are expected to be a single\n",
    "      # scalar, unlike model.loss, which is expected to be the loss per sample.\n",
    "      # Therefore, we reduce over all dimensions, regardless of the shape.\n",
    "      # We take the sum because (apparently) Keras will add this to the *post*\n",
    "      # `reduce_sum` (total) loss.\n",
    "      # TODO(b/126259176): Add end-to-end Keras/TFP test to ensure the API's\n",
    "      # align, particularly wrt how losses are aggregated (across batch\n",
    "      # members).\n",
    "        return tf.reduce_sum(kl, name='batch_total_kl_divergence')\n",
    "\n",
    "    return _fn\n",
    "\n",
    "def log_mixture_prior_prob(w, comp_weight, first_prior_scale, second_prior_scale):\n",
    "    # Applies the formula of Blundell et al 2015 \n",
    "    comp_1_dist = tfp.distributions.Normal(0.0, first_prior_scale)\n",
    "    comp_2_dist = tfp.distributions.Normal(0.0, second_prior_scale)\n",
    "    comp_1_weight = comp_weight   \n",
    "    return tf.reduce_sum(tf.math.log(comp_1_weight * comp_1_dist.prob(w) + (1 - comp_1_weight) * comp_2_dist.prob(w)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fcef3-d79c-4e07-a0cb-9b79c202d170",
   "metadata": {},
   "source": [
    "The following function builds the Scale Gaussian Mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c4068-1eb8-40e0-a048-6e0bc5c50c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dense_variational import Custom_dvi\n",
    "def build_bayesian_scale_mixture_model(hp):\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    # LSTM aspect\n",
    "    # Input layer\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(input_width, num_features)))\n",
    "\n",
    "    # Final LSTM layer    \n",
    "    model.add(LSTM(96))\n",
    "\n",
    "    # Bayesian aspect\n",
    "\n",
    "    # Using Scale Gaussian mixture prior!\n",
    "    # Each density is zero mean, but variance of first greater than second\n",
    "    # Need to choose: weighting and scales\n",
    "    # Weigting of components\n",
    "    components_weight = hp.Choice(\"mixture_weight\", [0.25, 0.5, 0.75])\n",
    "    # Variances of components\n",
    "    first_prior_scale = hp.Choice(\"first_prior_scale\", [0.1, 0.25, 0.5, 0.75, 1.])\n",
    "    second_prior_scale = hp.Choice(\"second_prior_scale\", [0.0001, 0.00025, 0.0005, 0.00075, 0.001])\n",
    "    \n",
    "\n",
    "    def prior_mvmg(kernel_size, bias_size, dtype=None):\n",
    "        n = kernel_size + bias_size\n",
    "        return tf.keras.Sequential([\n",
    "          tfp.layers.DistributionLambda(lambda t: tfd.MixtureSameFamily(\n",
    "              mixture_distribution=tfd.Categorical(\n",
    "              probs=[components_weight, (1-components_weight)]),\n",
    "              components_distribution=tfd.Normal(\n",
    "                  loc=[0, 0],       # Zero mean as described in paper\n",
    "                  scale=[first_prior_scale, second_prior_scale] # \n",
    "              ) \n",
    "          ))          \n",
    "        ])\n",
    "    prior_func = prior_mvmg\n",
    "\n",
    "    # Scale of posterior\n",
    "    posterior_scale = hp.Float('posterior_scale', min_value=0.0001, max_value=0.1)\n",
    "    def posterior_tune(kernel_size, bias_size, dtype=None):\n",
    "        n = kernel_size + bias_size\n",
    "        c = np.log(np.expm1(1.))\n",
    "        return tf.keras.Sequential([\n",
    "          tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "          tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "              tfd.Normal(loc=t[..., :n],\n",
    "                         scale = 1e-3 + posterior_scale * tf.nn.softplus(c + t[..., n:])),\n",
    "              reinterpreted_batch_ndims=1)),\n",
    "        ])\n",
    "    \n",
    "    # Give opportunity of adding one more dense variational layer\n",
    "    # Hidden DenseVariational layer\n",
    "    use_hidden_vi_layer = hp.Boolean(\"use_hidden_vi_layer\")\n",
    "    with hp.conditional_scope(\"use_hidden_vi_layer\", [True]):\n",
    "        if use_hidden_vi_layer:\n",
    "            hidden_layer_units = hp.Int(\"hidden_vi_layer_neurons\", min_value=2, max_value=64)\n",
    "            model.add(tfp.layers.DenseVariational(\n",
    "                hidden_layer_units, make_posterior_fn=posterior_tune, make_prior_fn=prior_func,\n",
    "                kl_weight=1/num_samples\n",
    "            ))\n",
    "    \n",
    "    model.add(\n",
    "        Custom_dvi(\n",
    "            units=1, make_posterior_fn=posterior_tune, make_prior_fn=prior_func, \n",
    "            kl_weight=1/num_samples,\n",
    "            comp_weight=components_weight, first_scale=first_prior_scale,\n",
    "            second_scale=second_prior_scale\n",
    "        )    \n",
    "    )\n",
    "    \n",
    "    learning_rate =  hp.Float('learning_rate', min_value=0.0001, max_value=0.01)\n",
    "    optimizer = Adam(learning_rate=learning_rate)      \n",
    "    metrics = [\"mse\", \"mae\", \"mape\", tf.keras.metrics.RootMeanSquaredError(), tf.keras.losses.Huber()]\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ead11c-4ca4-4e1f-945a-51599ea3538c",
   "metadata": {},
   "source": [
    "## Trial function\n",
    "\n",
    "As we use a cross-validation approach, we implemented a custom trial function that specifies the behavior of the search for each trial. The BNNs requires drawing Monte Carlo (MC) samples of the models to estimate their performance, exemplified in the for loop. For the LSTM tuning, this line can be dropped. The MC Dropout model requires setting the training parameter to True to achieve Dropout behavior when making predictions. \n",
    "\n",
    "The code runs through each specified window, builds the model, trains it and makes predictions. Then, the predictions are evaluated according to the desired evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfda20-dcce-4d05-be1c-599182509d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Specify evaluation metrics as a dictionary\n",
    "rmse_ = tf.keras.metrics.RootMeanSquaredError()\n",
    "huber_ = tf.keras.losses.Huber()\n",
    "ms = {\n",
    "    \"MSE\": tf.keras.metrics.mse, \"MAE\": tf.keras.metrics.mae, \"MAPE\": tf.keras.metrics.mape, \n",
    "    \"RMSE\": rmse_, \"Huber\": huber_\n",
    "}\n",
    "# Specify the number of epochs to run, and is using a BNN, the number of MC samplings of models to use\n",
    "num_epochs = 50\n",
    "num_mc_samples = 100\n",
    "from sklearn.metrics import mean_squared_error\n",
    "class BayesTuner_Dropout(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, folder, callbacks, *args, **kwargs):\n",
    "        # Create dictionary to store MSE results\n",
    "        fold_results = {\"val_mse\":[]}\n",
    "        print(\"Going over folds...\")\n",
    "        mse_list = []\n",
    "        for i, fold in enumerate(folder):  # Iterate over windows\n",
    "            print(f\"Starting fold {i+1}\")\n",
    "            # Get the Tensorflow Map Dataset instance of the current window\n",
    "            train, val = fold\n",
    "            # Build model using the current trials' set of hyperparameters\n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            # Fit the model\n",
    "            hist = model.fit(train, epochs=num_epochs, validation_data=val, verbose=0, callbacks=callbacks)\n",
    "            # Print the number of epochs\n",
    "            antall_epochs = len(hist.history[\"loss\"])\n",
    "            print(f\"Finished training in {antall_epochs} epochs\")\n",
    "            epochs_key = f\"num_epochs_{i}\"\n",
    "            fold_results[epochs_key] = antall_epochs\n",
    "            # Make first prediction to get size of array\n",
    "            print(\"Making predictions\")\n",
    "            # Convert the validation data into array as this is required if using the __call__ method for predictions \n",
    "            # in a Keras model\n",
    "            x_val = np.concatenate([x for x, y in val], axis=0)\n",
    "            # Make first prediction\n",
    "            pred = model(x_val, training=True).numpy().flatten()  # Flatten to prevent (num, 1)\n",
    "            # If tuning an LSTM model, the folowing code is not necessary. Simply use the prediction, and evaluate it for each \n",
    "            # desired metric\n",
    "            # Save predictions in a prediction array for each Monte Carlo sampling of the model\n",
    "            predictions = np.zeros(shape=(num_mc_samples, pred.shape[0]))\n",
    "            # Run 100 Monte Carlo sampling of the model to estimate the epistemic uncertainty\n",
    "            for j in range(1, num_mc_samples):  # Run 100 MC evaluations to get Model performance\n",
    "                # To achieve Dropout behavior during predictions, the training argument is set to True\n",
    "                predictions[j] = model(x_val, training=True).numpy().flatten()\n",
    "            # Get mean and standard deviation of predictions\n",
    "            predictions_mean = predictions.mean(axis=0)\n",
    "            predictions_std = predictions.std(axis=0)\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            # Get y_value\n",
    "            val_y = np.concatenate([y for x, y in val], axis=0).reshape(-1)\n",
    "            # Get losses for all metrics\n",
    "            for key, metric in ms.items():\n",
    "                score = metric(val_y, predictions_mean).numpy()\n",
    "                print(f\"{key}: {score:.5f}\")\n",
    "                new_key = f\"{key}_{i}\"\n",
    "                fold_results[new_key] = score\n",
    "                #fold_results[KeyedRef]\n",
    "                #fold_results[key].append(score)\n",
    "            mse_list.append(mean_squared_error(val_y, predictions_mean))\n",
    "\n",
    "\n",
    "            # Compute and save correlation\n",
    "            corr = np.corrcoef(np.abs(val_y-predictions_mean), predictions_std)[0, 1]\n",
    "            fold_results[f\"Corr_{i}\"] = corr\n",
    "            print(f\"Standard deviation/residuals correlation: {corr:.3f}\")\n",
    "            \n",
    "            # Save spread of standard deviation to assess how volatile the \n",
    "            # performance is\n",
    "            std_spread = predictions_std.max() - predictions_std.min()\n",
    "            print(f\"Standard deviation spread: {std_spread:.5f}\")\n",
    "            fold_results[f\"StdDev_{i}\"] = std_spread\n",
    "            # The model can be saved, but this eats up a lot of memory leading us to drop this feature of the\n",
    "            # tuning procedure\n",
    "            #self.save_model(trial.trial_id, model)\n",
    "            print()\n",
    "            # Delete the model to free up memory\n",
    "            del model\n",
    "        # Compute means for objective value \n",
    "        fold_results[\"val_mse\"] = np.mean(mse_list)\n",
    "        \n",
    "        # Update trial\n",
    "        self.oracle.update_trial(\n",
    "            trial.trial_id,\n",
    "            fold_results\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91736ea6-95cf-4d30-a1ef-ffbdc82bf3e2",
   "metadata": {},
   "source": [
    "Create `WindowGenerator` object using the `window_generator` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314cc4fe-001c-437c-babb-96027a748e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_generator import WindowGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b131ac-f92c-45ab-91f0-7e1fedd73eac",
   "metadata": {},
   "source": [
    "Create the windows. 30 lags are included in this demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896744d-6906-4186-a987-94593fabef20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_width = 30\n",
    "window = WindowGenerator(\n",
    "    input_width=input_width, label_width=1, shift=1, label_columns=['VIX'], \n",
    "    train_df=train_df, test_df=test_df, scale=MinMaxScaler)\n",
    "window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e06c0-749a-4557-90c0-02a7366b3f48",
   "metadata": {},
   "source": [
    "Store the folds using as list of Tensorflow `Map Datasets` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e4f01-f351-4dcc-b0fb-ed56b8beb8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder = window.folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c90b9-95ff-4c53-9097-dc00eee2d480",
   "metadata": {},
   "source": [
    "Run the search for 100 trials. Here, we use a `Callback` that stops the training if the model performance does not improve after a set number of epochs. The `patience` parameter controls this behavior, and the `start_from_epoch` allows the models to warm up before applying the callback. As BNNs can vary greatly for the initial epochs, this parameter helps prevent premature stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285d0d3-7275-4da3-9f2e-14504466579e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overwrite=True\n",
    "tuner = CVTuner_LSTM(oracle=kt.oracles.BayesianOptimizationOracle(objective='val_mse', max_trials=100), hypermodel=build_LSTM_model, directory=\"baseline\", project_name=\"lstm_regular\", overwrite=overwrite)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_mse', patience=10, start_from_epoch=10)\n",
    "\n",
    "tuner.search(folder=folder, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2e430-019f-41f1-aa23-403ba8a16e8f",
   "metadata": {},
   "source": [
    "# Display tuner results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9fb03-b145-4fe3-8290-205af2e669ba",
   "metadata": {},
   "source": [
    "Create dataframe for storing the results for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d6d2b-8eb7-4a53-91bf-77588fdc780c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mds = [f\"Model_{i+1}\" for i in range(5)]\n",
    "fds = [\"1\", \"2\", \"3\", \"4\", \"5\", \"Mean\"]\n",
    "iterables = [mds, fds]\n",
    "idx = pd.MultiIndex.from_product(iterables, names=[\"Model\", \"Fold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecab44f-aa8b-4210-8a15-a0d0efdccacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tunres = pd.DataFrame(index=idx, columns=[\"MSE\", \"MAE\", \"Huber\", \"Std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f035e-76ab-469c-a717-00b9378326ea",
   "metadata": {},
   "source": [
    "Iterate over each of the ten best models, and save their acquired metrics. Additionally, print the model hyperparameters and performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a280f6-a947-403f-9f12-1b6da8a79cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dictionary of overall best values\n",
    "best_values = {i:[np.inf, -1, 100] for i in tuner.oracle.get_best_trials()[0].metrics.metrics.keys()}\n",
    "\n",
    "# Get corr values and mse values for each fold\n",
    "mses = [f\"MSE_{i}\" for i in range(5)]\n",
    "corrs = [f\"Corr_{i}\" for i in range(5)]\n",
    "maes = [f\"MAE_{i}\" for i in range(5)]\n",
    "hubers =  [f\"Huber_{i}\" for i in range(5)]\n",
    "stds = [f\"StdDev_{i}\" for i in range(5)]\n",
    "\n",
    "# Iterate over 10 best trials\n",
    "for i, trial in enumerate(tuner.oracle.get_best_trials(num_trials=10)):\n",
    "    print(f\"Model rank: {i+1}\\tTrial id: {trial.trial_id}\")\n",
    "    print(f\"MSE: {trial.score:.4f}\")\n",
    "    print()\n",
    "    j = 0\n",
    "    for ms, corr, ma, hub, st in zip(mses, corrs, maes, hubers, stds):\n",
    "        mval = trial.metrics.get_last_value(ms)\n",
    "        cval = trial.metrics.get_last_value(corr)\n",
    "        maeval = trial.metrics.get_last_value(ma)\n",
    "        hubval = trial.metrics.get_last_value(hub)\n",
    "        stval = trial.metrics.get_last_value(st)\n",
    "        \n",
    "        fd = (f\"Model_{i+1}\", f\"{j+1}\")\n",
    "        tunres.loc[fd, :] = [mval, maeval, hubval, stval]\n",
    "\n",
    "        print(f\"Fold {j}:\\tMSE: {mval:.5f}\\tCorr: {cval:.4f}\\tMAE: {maeval:.4f}\\tHuber: {hubval:.4f}\\tStd: {stval:.4f}\")\n",
    "        j += 1\n",
    "    print()\n",
    "    print(trial.hyperparameters.values)\n",
    "    print(\"-\"*40)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3f9c6-9b44-4279-8144-2a37ca702fce",
   "metadata": {},
   "source": [
    "Save the mean value for each metric over all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a73321-d988-4f35-88cf-d8824034e176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in tunres.index.get_level_values(\"Model\").unique():\n",
    "    tunres.loc(axis=0)[m, \"Mean\"] = tunres.loc(axis=0)[m, fds[:-1]].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45741f73-76f4-4fa2-ae6f-642c7d1deb73",
   "metadata": {},
   "source": [
    "## Display best performance according to each metric\n",
    "For every metric, the values for each of the ten best models are sorted to show their corresponding ranking. For brevity purposes, only the `MSE`, `MAE`, and `Huber` metrics are used here. Additionally, only the mean performance of all folds are demonstrated. The `key` argument in the below functions can be used to inspect the performance for specific folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4385e-73e7-459b-8f83-827bdff6b489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tunres.xs(key=\"Mean\", level=\"Fold\").sort_values(by=\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151c2f4-a869-43a5-a939-406faffd76a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tunres.xs(key=\"Mean\", level=\"Fold\").sort_values(by=\"MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84f566-3789-4ad9-b244-8a4f9208a275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tunres.xs(key=\"Mean\", level=\"Fold\").sort_values(by=\"Huber\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-venv",
   "language": "python",
   "name": "master-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
